{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import csv\n",
    "from sklearn import tree\n",
    "import sys\n",
    "sys.stdout.flush()\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.table import Table\n",
    "import segment_cluster as sc\n",
    "import importlib\n",
    "importlib.reload(sc)\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cwd = os.getcwd()\n",
    "if cwd.split(\"/\")[1] == \"home\":\n",
    "    data_path=\"/home/jkok1g14/Documents/GRS1915+105/data/Std1_PCU2\"\n",
    "elif cwd.split(\"/\")[1] == \"export\":\n",
    "    data_path=\"/export/data/jakubok/GRS1915+105/Std1_PCU2\"\n",
    "else:\n",
    "    print(\"Set the path of data directory!\", Flush=True)\n",
    "\n",
    "#creates a dictionary of observation_ID: \"state\" items for labeled observations\n",
    "clean_belloni = open('1915Belloniclass_updated.dat')\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state\n",
    "\n",
    "#creates a list of labeled observations with abailable data\n",
    "available = []\n",
    "pool=[]\n",
    "#/home/jkok1g14/Documents/GRS1915+105/data\n",
    "#/export/data/jakubok/GRS1915+105/Std1_PCU2\n",
    "for root, dirnames, filenames in os.walk(data_path):\n",
    "    for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "        available.append(filename)\n",
    "for ob, state in ob_state.items():\n",
    "    if ob+\"_std1_lc.txt\" in available:\n",
    "        pool.append(ob)  \n",
    "\n",
    "#create a list of arrays with time and counts for the set of Belloni classified observations\n",
    "lc_dirs=[]\n",
    "lcs=[]\n",
    "ids=[]\n",
    "for root, dirnames, filenames in os.walk(data_path):    \n",
    "    for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "        if filename.split(\"_\")[0] in pool:\n",
    "            lc_dirs.append(os.path.join(root, filename))\n",
    "\n",
    "            \n",
    "#make 2D arrays for light curves, with columns of counts and time values\n",
    "for lc in lc_dirs:\n",
    "    ids.append(lc.split(\"/\")[-1].split(\"_\")[0])\n",
    "    time, counts, err = np.loadtxt(lc).T\n",
    "    \n",
    "    \n",
    "\n",
    "    ###1s binning and time check to eliminate points outside of GTIs\n",
    "    f8t = np.mean(time[:(len(time)//8)*8].reshape(-1, 8), axis=1)\n",
    "    weights=err**-2\n",
    "    f8c = np.average(counts[:(len(counts)//8)*8].reshape(-1, 8), axis=1, weights=weights[:(len(weights)//8)*8].reshape(-1, 8))\n",
    "    f8e = np.sqrt(1/np.sum(weights[:(len(weights)//8)*8].reshape(-1, 8), axis=1))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([114.26285486, 117.57550765, 114.26285486, ..., 125.47509713,\n",
       "       134.10443691, 130.23056477])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    rm_points = []\n",
    "    skip=False\n",
    "    for i in range(len(f8t)-1):\n",
    "        if skip==True:\n",
    "            skip=False\n",
    "            continue\n",
    "        delta = f8t[i+1]-f8t[i]\n",
    "        if delta > 1.0:\n",
    "            rm_points.append(i+1)\n",
    "            skip=True   \n",
    "    times=np.delete(f8t,rm_points)\n",
    "    counts=np.delete(f8c,rm_points)\n",
    "    lcs.append(np.stack((times,counts)))\n",
    "#a list of light curve 2D arrays\n",
    "\n",
    "lc_classes=[]\n",
    "for i in ids:\n",
    "    lc_classes.append(ob_state[i])\n",
    "\n",
    "drop_classes=[]\n",
    "for clas, no in Counter(lc_classes).items():\n",
    "    if no<7:\n",
    "        drop_classes.append(clas)\n",
    "\n",
    "lcs_abu = []\n",
    "classes_abu = []\n",
    "ids_abu = []\n",
    "for n, lc in enumerate(lc_classes):\n",
    "    if lc not in drop_classes:\n",
    "        classes_abu.append(lc)\n",
    "        lcs_abu.append(lcs[n])\n",
    "        ids_abu.append(ids[n])  \n",
    "#a list of light curve 2D arrays of classes with at least 7 light curves\n",
    "x_train, x_test, y_train, y_test, id_train, id_test = train_test_split(lcs_abu, classes_abu, ids_abu, test_size=0.25, stratify=classes_abu)\n",
    "\n",
    "rho_file=np.loadtxt(\"synthetic_rhos.csv\", delimiter=',')\n",
    "rho_train, rho_valid, rho_train_ids, rho_valid_ids= train_test_split(rho_file, list(range(len(rho_file))) ,test_size=0.25)\n",
    "\n",
    "#lists for the validation results \n",
    "#numbers of clusters and segment lengths to be tested\n",
    "reco_error=[]\n",
    "#reco_classes=[]\n",
    "k_clusters=[50]\n",
    "seg_lens=[8]\n",
    "classes=list(set(y_train))\n",
    "print(classes, flush=True)\n",
    "for k_id, k_cluster in enumerate(k_clusters):\n",
    "    for len_id, seg_len in enumerate(seg_lens):\n",
    "        # calculate the slide values\n",
    "        seg_slides=[1]\n",
    "        for slide_id, seg_slide in enumerate(seg_slides):\n",
    "            #leave one out cross validation\n",
    "            \n",
    "            \n",
    "            \n",
    "            ##train the model\n",
    "            #loop throught the light curves of a given class and segments them\n",
    "            all_train_segments=[]\n",
    "            for rho in rho_train:\n",
    "                train_segments=sc.segmentation(rho, seg_len, seg_slide, time_stamps=False)\n",
    "                all_train_segments.append(train_segments)\n",
    "            all_train_segments=np.vstack(all_train_segments)\n",
    "            #cluster the segments\n",
    "            cluster=KMeans(n_clusters=k_cluster, random_state=0)\n",
    "            cluster.fit(all_train_segments)\n",
    "\n",
    "            ### reconstruction: complete for every validation observation in leave-one-out. Then do once for lcs of all other classes\n",
    "            for n_rho, rho in enumerate(rho_valid):\n",
    "                valid_segments= sc.segmentation(rho, seg_len, seg_len , time_stamps=False)\n",
    "                reco = sc.reconstruct(valid_segments, rho, cluster, rel_offset=False, seg_slide=seg_len)\n",
    "                error=np.sqrt(np.mean((rho[seg_len:-seg_len]-reco[seg_len:-seg_len])**2))\n",
    "                reco_error.append((k_id,len_id,slide_id,len(classes)+1, n_rho, error))\n",
    "                print((k_id,len_id,slide_id,len(classes), n_rho, error), flush=True)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #reconstruction loop through light curves for every class other than rho              \n",
    "            for n_valid, valid_class in enumerate(classes):\n",
    "                validation_ids=np.where(np.array(y_train)=='{}'.format(valid_class))[0]\n",
    "                for ts_id in validation_ids:\n",
    "                    valid_ts=x_train[ts_id]\n",
    "                    valid_segments= sc.segmentation(valid_ts, seg_len, seg_len , time_stamps=True)\n",
    "                    reco = sc.reconstruct(valid_segments, valid_ts, cluster, rel_offset=False, seg_slide=seg_len)\n",
    "                    error=np.sqrt(np.mean((valid_ts[1][seg_len:-seg_len]-reco[1][seg_len:-seg_len])**2))\n",
    "                    reco_error.append((k_id,len_id,slide_id,n_valid,int(id_train[ts_id].replace(\"-\",\"\")), error))\n",
    "                    print((k_id,len_id,slide_id,n_valid,int(id_train[ts_id].replace(\"-\",\"\")), error))\n",
    "reco_error_ar=np.array(reco_error)\n",
    "print(classes, flush=True)\n",
    "np.savetxt(\"valid_results_20190430.csv\", reco_error_ar, delimiter=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold\n",
    "period = np.pi # period (must be known already!)\n",
    "foldTimes = time / period # divide by period to convert to phase\n",
    "foldTimes = foldTimes % 1 # take fractional part of phase only (i.e. disc\n",
    "ard whole number part)\n",
    "# plot folded lightcurve\n",
    "plt.errorbar(foldTimes,flux,yerr=err,linestyle='none',marker='o')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# bin\n",
    "nbins = 30\n",
    "# chosen number of bins across the period\n",
    "width = 1.0/float(nbins) # calculate the width of the bins; total lightcurve length/no. of bins\n",
    "\n",
    "# create arrays for bin values and weights\n",
    "bins = np.zeros(nbins)\n",
    "weights = np.zeros(nbins)\n",
    "\n",
    "# bin!\n",
    "for i in range(len(flux)):\n",
    "    n = int(foldTimes[i] / width) # calculate bin number for this value; finds the index of one of the 30 bins\n",
    "    weight = err[i]**-2.0 # calculate weight == error^-2\n",
    "    bins[n] += flux[i] * weight # add weighted value to bin (value timesweight)\n",
    "    weights[n] += weight# add weight to bin weights\n",
    "    \n",
    "\n",
    "counts*err**-2\n",
    "\n",
    "bins /= weights # normalise weighted values using sum of weights\n",
    "\n",
    "binErr = np.sqrt(1.0/(weights)) # calculate bin errors from squared weights\n",
    "\n",
    "binEdges = np.arange(nbins) * width # create array of bin edge values for plotting\n",
    "\n",
    "plt.errorbar(binEdges,bins,yerr=binErr,linestyle='none',marker='o') #plot binned lightcurve\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
